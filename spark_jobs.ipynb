{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eaf6e07-679c-48ad-98bb-48b47a6cfcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c547c06ec866:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_job</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffff82fc430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparksession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark=SparkSession.builder.appName(\"spark_job\").master(\"local[*]\").getOrCreate()\n",
    "spark\n",
    "\n",
    "\n",
    "#local[*] takes all the cores present in the machine, you cant specify number of executors in local mode\n",
    "#For local development, Spark runs in a single executor but can process tasks in parallel across multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373d1d85-0a7a-4f77-a964-52b4ba13b9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: 10\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(\"Number of cores:\", sc.defaultParallelism) \n",
    "\n",
    "#it gives number of cores in the machine as i used local[*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21d73ad-3c21-46e6-b87f-d0ef837da801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading parquet files\n",
    "df_transactions = spark.read.parquet(\"spark-experiments/data/data_skew/transactions.parquet\")\n",
    "\n",
    "#job-1: creates a job, eventhough the action is not called here it creates a job since it reads only metadata\n",
    "#stages: 1/1\n",
    "#tasks: 1/1\n",
    "#input : None\n",
    "#output: None\n",
    "#shuffle_read: None\n",
    "#shuffle_write: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b71abcd-966b-4237-b25e-829bd3f2d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transactions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2153bfe4-bb68-4930-8811-e52789cd6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|        city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|      boston|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|    portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|     chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97| los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|     chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T1SMX9EUG21BBSE|2015-02-11|2015|    2| 11|    Education| 54.14|    portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T449R5YAV3BMX7O|2012-11-14|2012|   11| 14|     Gambling| 88.34|     seattle|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZHEHBKEXF1TPS2|2016-11-19|2016|   11| 19|    Groceries| 95.69|philadelphia|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TVYNJ1ZYAZI6O6J|2013-05-02|2013|    5|  2|     Gambling| 50.53|    portland|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TMRZVDMBXYVOYSH|2011-09-03|2011|    9|  3|          Tax|228.39| los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T16BPI2Q485F7XF|2015-08-19|2015|    8| 19|     Gambling| 59.75|      denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T33AQQUDNT3ND4E|2018-05-05|2018|    5|  5|    Groceries|175.51| los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TLJOSHHDQ73IZM4|2011-04-10|2011|    4| 10| Motor/Travel|  19.4| los_angeles|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|THHTJFKWB7AT3AF|2014-08-24|2014|    8| 24|    Groceries| 61.67|   san_diego|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T0XX1VCPQXK0XPZ|2014-06-14|2014|    6| 14|Entertainment|  2.37|    new_york|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TSD2WUKMJPBXYTL|2011-04-01|2011|    4|  1|    Education|790.15|     chicago|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T1FSOGKASVCV7FM|2017-07-14|2017|    7| 14|Entertainment|  4.01|      denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|T93196UFIQM5E27|2012-08-03|2012|    8|  3|    Education| 41.57|philadelphia|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TGO6A5NNOMI6IT5|2016-04-21|2016|    4| 21|Entertainment|  2.51|      denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQ0KP2CUAB1B9IL|2018-04-01|2018|    4|  1|Entertainment|  2.82| los_angeles|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show()\n",
    "\n",
    "#job-1: creates a job since action is called\n",
    "#stages: 1/1\n",
    "#tasks: 1/1\n",
    "#input : 7.6 MiB  (1 MiB= 1024 KiB, 1MB= 1000 KB) [refers to the amount of data read into Spark for processing in this job.]\n",
    "#output: None\n",
    "#shuffle_read: None\n",
    "#shuffle_write: None\n",
    "\n",
    "\n",
    "#dag details\n",
    "# Scan parquet\n",
    "\n",
    "# number of files read: 163\n",
    "# scan time: 149 ms\n",
    "# metadata time: 82 ms\n",
    "# size of files read: 862.7 MiB\n",
    "# number of output rows: 4,096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed0fde2-5469-4ed1-9592-5ff45e09b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading parquet files\n",
    "df_customers = spark.read.parquet(\"spark-experiments/data/data_skew/customers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dece0cd2-0927-4e95-b221-4c11eb81a7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "|   cust_id|          name|age|gender|  birthday|  zip|        city|\n",
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "|C007YEYTX9|  Aaron Abbott| 34|Female| 7/13/1991|97823|      boston|\n",
      "|C00B971T1J|  Aaron Austin| 37|Female|12/16/2004|30332|     chicago|\n",
      "|C00WRSJF1Q|  Aaron Barnes| 29|Female| 3/11/1977|23451|      denver|\n",
      "|C01AZWQMF3| Aaron Barrett| 31|  Male|  7/9/1998|46613| los_angeles|\n",
      "|C01BKUFRHA|  Aaron Becker| 54|  Male|11/24/1979|40284|   san_diego|\n",
      "|C01RGUNJV9|    Aaron Bell| 24|Female| 8/16/1968|86331|      denver|\n",
      "|C01USDV4EE|   Aaron Blair| 35|Female|  9/9/1974|80078|    new_york|\n",
      "|C01WMZQ7PN|   Aaron Brady| 51|Female| 8/20/1994|52204|philadelphia|\n",
      "|C021567NJZ|  Aaron Briggs| 57|  Male| 3/10/1990|22008|philadelphia|\n",
      "|C023M6MKR3|   Aaron Bryan| 29|  Male| 4/10/1976|05915|philadelphia|\n",
      "|C0248N0EK3|  Aaron Burton| 26|Female| 8/27/1964|50477| los_angeles|\n",
      "|C02C54RPNL|  Aaron Burton| 46|  Male| 5/29/1976|75857|     seattle|\n",
      "|C02ERIY1O4|  Aaron Cannon| 50|  Male| 5/23/1965|70209|    portland|\n",
      "|C02EVK2JWT|  Aaron Carter| 36|Female| 6/21/1993|89011|      denver|\n",
      "|C02JNTM46B|Aaron Chambers| 51|  Male|  1/6/2001|63337|    new_york|\n",
      "|C030A69V1L|  Aaron Clarke| 55|  Male| 4/28/1999|77176|philadelphia|\n",
      "|C033JBNUYU|Aaron Ferguson| 27|  Male| 6/21/1959|73150|      denver|\n",
      "|C034RB2MQ6|    Aaron Ford| 63|  Male|  7/8/1988|90592|     chicago|\n",
      "|C036GAJ3BV|Aaron Franklin| 22|Female| 3/14/1961|01187|   san_diego|\n",
      "|C03U340T3R| Aaron Gardner| 59|Female| 3/18/1975|31502|      denver|\n",
      "+----------+--------------+---+------+----------+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers.show()\n",
    "df_customers.count() #gives number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba37dd0a-1456-48ed-b0be-edd4fa8ac756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ee664-c059-4a2a-a0a0-4e40183d6592",
   "metadata": {},
   "source": [
    "# Narrow Transformations\n",
    "- `filter` rows where `city='boston'`\n",
    "- `add` a new column: adding `first_name` and `last_name`\n",
    "- `alter` an exisitng column: adding 5 to `age` column\n",
    "- `select` relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b51558-f515-45af-b9f4-b60bf08c6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_narrow_transformations=(\n",
    "df_customers\n",
    " .filter(F.col(\"city\")==\"boston\")\n",
    " .withColumn(\"age_plus_5\",F.col(\"age\")+5)\n",
    "  .select(\"cust_id\",\"name\",\"age\",\"gender\"))\n",
    "\n",
    "#it is treated as 1 job, 1 stage and 1 task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13a460e-7ca2-4e3e-81a8-deaa1707b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing back to parquet\n",
    "df_narrow_transformations.write.format(\"parquet\").mode(\"overwrite\").save(\"spark-experiments/data/data_skew/modified.parquet\")\n",
    "\n",
    "#job details \n",
    "#input - 143.2 KiB [read whole data customer files]\n",
    "#output - 11.9 KiB [after filteration the size of data taken as output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8c5d-de6b-4d89-928b-5b0a375e8a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c7564d-3b6a-401b-b06e-88964b494730",
   "metadata": {},
   "source": [
    "# Wide Transformations\n",
    "1. Joins\n",
    "   - Sort Merge Join\n",
    "   - Broadcast Join\n",
    "2. GroupBy\n",
    "   - `count`\n",
    "   - `countDistinct`\n",
    "   - `sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2af9f3-6416-438a-9588-e3496949b108",
   "metadata": {},
   "source": [
    "### Sort merge join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da542f83-593e-414d-9b72-4ecc26955d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)  \n",
    "\n",
    "#spark.sql.autoBroadcastJoinThreshold: This configuration option controls the maximum size of a DataFrame (in bytes) that Spark is allowed to broadcast when performing a join.\n",
    "#By default, this value is set to 10MB (10 * 1024 * 1024 bytes), meaning that Spark will automatically broadcast DataFrames smaller than this size during a join operation.\n",
    "\n",
    "#Setting it to -1: By setting this value to -1, you are effectively disabling automatic broadcasting of any DataFrame, regardless of its size. \n",
    "#This means Spark will not automatically broadcast any DataFrames during joins, and you will need to manually specify when to use a broadcast join (via F.broadcast()), even for small DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a641a70-ca47-4833-9f5f-a72b824c1228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sort_merge= df_transactions.join(df_customers, on='cust_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9997e628-9997-4e15-b179-3c5579b31724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958\n",
      "5000\n",
      "2958\n"
     ]
    }
   ],
   "source": [
    "unique_cust_ids_transactions = df_transactions.select(\"cust_id\").distinct()\n",
    "unique_cust_ids_customers = df_customers.select(\"cust_id\").distinct()\n",
    "common_cust_ids = unique_cust_ids_transactions.intersect(unique_cust_ids_customers)\n",
    "print(unique_cust_ids_transactions.count())\n",
    "print(unique_cust_ids_customers.count())\n",
    "print(common_cust_ids.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d668c9f-ca24-44e6-934d-93acab704d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-------+-------------+------------+---+------+---------+-----+------+\n",
      "|   cust_id|start_date|end_date|         txn_id|      date|year|month|day| expense_type|    amt|         city|        name|age|gender| birthday|  zip|  city|\n",
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-------+-------------+------------+---+------+---------+-----+------+\n",
      "|C00WRSJF1Q|2012-11-01|    null|TXNU40MYVB3QXBU|2018-11-01|2018|   11|  1| Motor/Travel|2129.82|    san_diego|Aaron Barnes| 29|Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|    null|TKGK0XNNTDI0MPX|2014-08-06|2014|    8|  6|    Groceries| 126.65|       boston|Aaron Barnes| 29|Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|    null|T1QLRMJWEYOP8YD|2015-09-10|2015|    9| 10|Entertainment|  28.94|     new_york|Aaron Barnes| 29|Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|    null|T7YCEUYHV6FCVRR|2020-06-25|2020|    6| 25|    Education|  21.34|    san_diego|Aaron Barnes| 29|Female|3/11/1977|23451|denver|\n",
      "|C00WRSJF1Q|2012-11-01|    null|T5QYMWG3TDXF7HK|2018-12-26|2018|   12| 26|Entertainment|  22.43|san_francisco|Aaron Barnes| 29|Female|3/11/1977|23451|denver|\n",
      "+----------+----------+--------+---------------+----------+----+-----+---+-------------+-------+-------------+------------+---+------+---------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sort_merge.show(5)\n",
    "#3 jobs will be created here\n",
    "#job -1: stages 1, task 12 [since df_transactions.rdd.getNumPartitions() is 12] ,reads the entire data from df_transactions [input: 871.6 MiB] and then shuffles the data [shuffle write : 2.4 GiB] \n",
    "#job -2:stages 1, task 1 [since df_customers.rdd.getNumPartitions() is 1], reads the entire data from df_customers [input: 157.2 KiB] and then shuffles the data [shuffle write :383.8 KiB]\n",
    "#job -3: stages 1, task 1, executes join , shuffle read: 60.0 MiB. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba15882-bd12-45a2-9eaa-4ecb6f299077",
   "metadata": {},
   "source": [
    "### Broadcast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adccec0a-2a4a-432b-97e3-0270d9a490d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)\n",
    "\n",
    "#This configures the maximum size (in bytes) for a table to be eligible for broadcast joins. In this example:\n",
    "#10485760 bytes = 10 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff0cde7-58ee-4ec6-addf-d2300ffdede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_broadcast_transations=(\n",
    "    df_transactions.join(F.broadcast(df_customers),on='cust_id',how='inner'))\n",
    "\n",
    "#Even though the broadcast join optimizes the shuffling by broadcasting a small table to all worker nodes, it is still a wide transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4b516c1-451a-4896-910c-0645a79197ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------+---+------+---------+-----+------+\n",
      "|   cust_id|start_date|  end_date|         txn_id|      date|year|month|day| expense_type|   amt|       city|    name|age|gender| birthday|  zip|  city|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------+---+------+---------+-----+------+\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TZ5SMKZY9S03OQJ|2018-10-07|2018|   10|  7|Entertainment| 10.42|     boston|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYIAPPNU066CJ5R|2016-03-27|2016|    3| 27| Motor/Travel| 44.34|   portland|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TETSXIK4BLXHJ6W|2011-04-11|2011|    4| 11|Entertainment|  3.18|    chicago|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TQKL1QFJY3EM8LO|2018-02-22|2018|    2| 22|    Groceries|268.97|los_angeles|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "|C0YDPQWPBJ|2010-07-01|2018-12-01|TYL6DFP09PPXMVB|2010-10-16|2010|   10| 16|Entertainment|  2.66|    chicago|Ada Lamb| 32|Female|9/29/2005|22457|denver|\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+-----------+--------+---+------+---------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_broadcast_transations.show(5)\n",
    "\n",
    "#2 jobs are created here \n",
    "#job -1 : job name - broadcast exchange - 1 stage , 1 task [only 1 partition] reads all the customers data. Input - 157.2 KiB, no shuffling of data here\n",
    "#job -2: after broadcast exchange - the smaller df is sent for BroadcastHashJoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f7d7d-ed09-4da0-8a5c-a8439e28c030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "899f158f-64cc-4956-8a17-64da721011fd",
   "metadata": {},
   "source": [
    "### GroupBy Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568aa368-92d6-4f78-a303-473d3fd0472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_counts=(df_transactions.groupBy(\"city\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768c6eb3-c98a-448e-b22b-854fb26bbb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|     city|  count|\n",
      "+---------+-------+\n",
      "|san_diego|3977780|\n",
      "|  chicago|3979023|\n",
      "|   denver|3980274|\n",
      "|   boston|3978268|\n",
      "|  seattle|3980022|\n",
      "+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_counts.show(5)\n",
    "\n",
    "#2 jobs is created here\n",
    "\n",
    "#job-1 : it is to read all the data of df_transactions data, input - 21.7 MiB, shuffle write - 8.4 KiB\n",
    "\n",
    "#job-2: HashAggregate happens here, shuffle read - 8.4 KiB\n",
    "#2 hashaggregate happens here [1st before shuffle and 2nd after shuffle]\n",
    "#hash aggregate happens in each partition i.e, [partition 1 has A,A,B cities hashaggregate give A-2, B-1, partition 2 has A,A,A,B hashaggreagte gives A-3, B-1] \n",
    "#once it is done it shuffles the data like same key under same partition (if data is less then AQE will coalesce all the partitions to 1)\n",
    "#Now hashaggregate happens again to give final count since A-2,A-3 will combine to give A-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e1c76d1-959d-433c-b646-876a6aedc705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- expense_type: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659bafb-ebeb-4a18-a390-99519618320d",
   "metadata": {},
   "source": [
    "### GroupBy Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff5089d-dcd1-488a-83e4-1bf01b8549c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_amt_sum=(df_transactions.groupBy(\"city\").agg(F.sum(\"amt\").alias(\"txn_amt\")))\n",
    "\n",
    "#have to agg function since amt in the schema is a string or can directly use sum('amt') if not in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ba9c4e2-7aa2-405e-b125-ddb0ff145a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|     city|             txn_amt|\n",
      "+---------+--------------------+\n",
      "|san_diego| 3.297982686000007E8|\n",
      "|  chicago|3.2988120044000095E8|\n",
      "|   denver| 3.298814956400023E8|\n",
      "|   boston| 3.301009563300014E8|\n",
      "|  seattle|3.3019513776999813E8|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_txn_amt_sum.show(5)\n",
    "\n",
    "# 2 jobs will be created here\n",
    "#same as groupby Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c6dd1-ea06-4353-b933-a3836b6e14db",
   "metadata": {},
   "source": [
    "### GroupBy Count distinct       \n",
    "[different job pattern compared to above two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "417f8618-93c1-4b5e-97c8-4641a7d9876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_per_city=(df_transactions.groupBy(\"cust_id\").agg(F.countDistinct(\"city\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ef5179-4b56-4f63-bd43-bc50343f1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|   cust_id|count(city)|\n",
      "+----------+-----------+\n",
      "|CPP8BY8U93|         10|\n",
      "|CYB8BX9LU1|         10|\n",
      "|CFRT841CCD|         10|\n",
      "|CA0TSNMYDK|         10|\n",
      "|COZ8NONEVZ|         10|\n",
      "|C46OCVH3WG|         10|\n",
      "|C1QF29WCA6|         10|\n",
      "|CTJBQB0OJ1|         10|\n",
      "|CD0DXL8XTM|         10|\n",
      "|CADBQ5OL5C|         10|\n",
      "|CUCQ9LBQWW|         10|\n",
      "|C3NH8CDGWM|         10|\n",
      "|CEEPXNQ9NQ|         10|\n",
      "|C7ALJDG81A|         10|\n",
      "|CUDKFKPAFB|         10|\n",
      "|C2L2984OZK|         10|\n",
      "|CDDRDAEY13|         10|\n",
      "|CIZT509YVA|         10|\n",
      "|CSTJ6YYXE3|         10|\n",
      "|CW1X1V0PRG|         10|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_txn_per_city.show()\n",
    "# 2 jobs re created here\n",
    "#job - 1 : it is to read all the data of df_transactions data, input - 21.7 MiB, shuffle write - 8.4 KiB\n",
    "#job - 2: HashAggregate happens here, shuffle read - 8.4 KiB\n",
    "#You should always observe on what keys and function that hashaggregate is happening \n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2829a4-069e-4bab-8558-da0c15567f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "window functions\n",
    "pivot\n",
    "\n",
    "\n",
    "example syntax:\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(\"month\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window_desc = Window.partitionBy(\"name\").orderBy(F.col(\"month\").desc())\n",
    "\n",
    "\n",
    "df.groupBy('').agg(F.sum('')).orderBy(f.col('month').desc())\n",
    "\n",
    "\n",
    "\n",
    "df_with_running_total = df.withColumn(\"running_total\", F.sum(\"sales\").over(window_spec))\n",
    "df_with_running_total.show()\n",
    "\n",
    "pivot_df = df.groupBy(\"name\").pivot(\"month\").sum(\"sales\")\n",
    "pivot_df.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
